{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from visdom import Visdom\n",
    "\n",
    "from lib.VisdomWrapper import *\n",
    "from lib.DataManager import *\n",
    "from lib.GANs import *\n",
    "from lib.DataCreationWrapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Options\n",
    "bal_raw = False\n",
    "bal_syn = False\n",
    "bal_aug = False\n",
    "zer_raw = False\n",
    "zer_syn = False\n",
    "zer_aug = False\n",
    "nin_raw = True\n",
    "nin_syn = True\n",
    "nin_aug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "batch_size = 512\n",
    "num_epochs = 30\n",
    "img_width = 28 #hardcoded\n",
    "n_features = img_width**2\n",
    "n_noise_features = 100\n",
    "n_classes = 10\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "vis = VisdomController()\n",
    "\n",
    "mnist = dset.MNIST('input', train=True, download=True, transform=T.ToTensor())\n",
    "mnist_high_nine = get_unbalanced_mnist([.1, .1, .1, .1, .1, .1, .1, .1, .1, 1], batch_size=batch_size)\n",
    "\n",
    "mnist_test=dset.MNIST('input', train=False, download=True, transform=T.ToTensor())\n",
    "balanced_test = DataLoader(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_balance(labels):\n",
    "    cnts = torch.bincount(labels)\n",
    "    max = torch.max(cnts)\n",
    "    return (max * torch.ones(len(cnts)) - cnts).cpu().numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "with torch.no_grad():\n",
    "    if bal_raw or bal_aug:\n",
    "        bal_train = DataLoader(mnist, batch_size =1000)\n",
    "        X, Y = data_loader_to_tensor(bal_train)\n",
    "        bal_train = DataLoader(TensorDataset(X, Y), batch_size=batch_size, shuffle=True)\n",
    "        X, Y = None, None\n",
    "        \n",
    "    if zer_raw or zer_aug:\n",
    "        zer_train = get_unbalanced_mnist([.01, .1, .1, .1, .1, .1, .1, .1, .1, .1], batch_size=1000)\n",
    "        X, Y = data_loader_to_tensor(zer_train)\n",
    "        zer_train = DataLoader(TensorDataset(X, Y), batch_size=batch_size, shuffle=True)\n",
    "        X, Y = None, None\n",
    "        \n",
    "    if nin_raw or nin_aug:\n",
    "        nin_train = get_unbalanced_mnist([.1, .1, .1, .1, .1, .1, .1, .1, .1, 1], batch_size=1000)\n",
    "        X, Y = data_loader_to_tensor(nin_train)\n",
    "        nin_train = DataLoader(TensorDataset(X, Y), batch_size=batch_size, shuffle=True)\n",
    "        X, Y = None, None\n",
    "    \n",
    "    if bal_syn or bal_aug:\n",
    "        bal_gen = Conv_GeneratorNetwork(n_noise_features, n_features, n_classes)\n",
    "        bal_gen.load_state_dict(torch.load(\"models\\gen_nn_bal\"))\n",
    "        bal_gen = bal_gen.cuda()\n",
    "        bal_gen.eval()\n",
    "        if bal_syn:\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(bal_gen, gaussian_noise, 6000 * np.ones(10).astype('int'))\n",
    "            syn_bal_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None\n",
    "        if bal_aug:\n",
    "            print(\"Augmenting bal\")\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(bal_gen, gaussian_noise, calc_balance(bal_train.dataset.tensors[1]) * np.ones(10).astype('int'))\n",
    "            synth_data = torch.cat((synth_data.view(len(synth_labels), 1, 28, 28).cuda(), bal_train.dataset.tensors[0].cuda()), dim=0)\n",
    "            synth_labels = torch.cat((synth_labels.cuda(), bal_train.dataset.tensors[1].cuda()))\n",
    "            aug_bal_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None\n",
    "\n",
    "    if zer_syn or zer_aug:\n",
    "        zer_gen = Conv_GeneratorNetwork(n_noise_features, n_features, n_classes)\n",
    "        zer_gen.load_state_dict(torch.load(\"models\\gen_nn_low_zero\"))\n",
    "        zer_gen.cuda()\n",
    "        zer_gen.eval()\n",
    "        if zer_syn:\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(zer_gen, gaussian_noise, 6000 * np.ones(10).astype('int'))\n",
    "            syn_low_zero_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None\n",
    "        if zer_aug:\n",
    "            print(\"Augmenting zer\")\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(zer_gen, gaussian_noise, calc_balance(zer_train.dataset.tensors[1]) * np.ones(10).astype('int'))\n",
    "            synth_data = torch.cat((synth_data.view(len(synth_labels), 1, 28, 28).cuda(), zer_train.dataset.tensors[0].cuda()), dim=0)\n",
    "            synth_labels = torch.cat((synth_labels.cuda(), zer_train.dataset.tensors[1].cuda()))\n",
    "            aug_zer_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None\n",
    "\n",
    "    if nin_syn or nin_aug:\n",
    "        nin_gen = Conv_GeneratorNetwork(n_noise_features, n_features, n_classes)\n",
    "        nin_gen.load_state_dict(torch.load(\"models\\gen_nn_high_nine\"))\n",
    "        nin_gen.cuda()\n",
    "        nin_gen.eval()\n",
    "        if nin_syn:\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(nin_gen, gaussian_noise, 6000 * np.ones(10).astype('int'))\n",
    "            syn_nin_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None\n",
    "        if nin_aug:\n",
    "            print(\"Augmenting nin\")\n",
    "            synth_data, synth_labels = synthesize_data_of_each_label(nin_gen, gaussian_noise, calc_balance(nin_train.dataset.tensors[1]) * np.ones(10).astype('int'))\n",
    "            synth_data = torch.cat((synth_data.view(len(synth_labels), 1, 28, 28).cuda(), nin_train.dataset.tensors[0].cuda()), dim=0)\n",
    "            synth_labels = torch.cat((synth_labels.cuda(), nin_train.dataset.tensors[1].cuda()))\n",
    "            aug_nin_data_loader = DataLoader(TensorDataset(synth_data.view(len(synth_labels), 1, 28, 28), synth_labels), batch_size=batch_size, shuffle=True)\n",
    "            synth_data, synth_labels = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency(data_loader, title):\n",
    "    x = np.arange(10)\n",
    "    cnts = torch.zeros(10)\n",
    "    for _, y in data_loader:\n",
    "        cnts += torch.bincount(y.cpu())\n",
    "    vis.CreateStaticBarPlot(cnts, x, title, \"Class\", \"Count\", \"FreqPlot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=5, stride=1),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4*4*64, 4*4*64, bias=True),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.Linear(4*4*64, 10, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, optimizer, data, key=\"Loss\"):\n",
    "    for epoch in range(num_epochs):\n",
    "        for n_batch, (x, y) in enumerate(data):\n",
    "            if len(x) != batch_size:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            scores = classifier(x)\n",
    "            out = loss(scores, y)\n",
    "            out.backward()\n",
    "            optimizer.step()\n",
    "        display.clear_output(True)\n",
    "        print(\"Epoch {}, {} / {}\".format(epoch, n_batch, len(data)))\n",
    "        print(\"Loss: \", out.item())\n",
    "        vis.loss_axis = epoch\n",
    "        vis.PlotLoss(key, out.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    print('Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_raw:\n",
    "    balanced_net = build_classifier().cuda()\n",
    "    balanced_net.train()\n",
    "    train_classifier(balanced_net, get_optimizer(balanced_net), bal_train, key=\"Balanced Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_aug:\n",
    "    balanced_aug_net = build_classifier().cuda()\n",
    "    balanced_aug_net.train()\n",
    "    train_classifier(balanced_aug_net, get_optimizer(balanced_aug_net), aug_bal_data_loader, key=\"Balanced Aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_syn:\n",
    "    balanced_syn_net = build_classifier().cuda()\n",
    "    balanced_syn_net.train()\n",
    "    train_classifier(balanced_syn_net, get_optimizer(balanced_syn_net), syn_bal_data_loader, key=\"Balanced Synth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_raw:\n",
    "    device = torch.device(\"cuda\")\n",
    "    balanced_net.eval()\n",
    "    test(balanced_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_aug:\n",
    "    device = torch.device(\"cuda\")\n",
    "    balanced_aug_net.eval()\n",
    "    test(balanced_aug_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bal_syn:\n",
    "    device = torch.device(\"cuda\")\n",
    "    balanced_syn_net.eval()\n",
    "    test(balanced_syn_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zer_raw:\n",
    "    low_zero_net = build_classifier().cuda()\n",
    "    low_zero_net.train()\n",
    "    train_classifier(low_zero_net, get_optimizer(low_zero_net), zer_train, key=\"Low Zero Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if zer_syn:\n",
    "    low_zero_syn_net = build_classifier().cuda()\n",
    "    low_zero_syn_net.train()\n",
    "    train_classifier(low_zero_syn_net, get_optimizer(low_zero_syn_net), syn_low_zero_data_loader, key=\"Low Zero Synth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zer_aug:\n",
    "    low_zero_aug_net = build_classifier().cuda()\n",
    "    low_zero_aug_net.train()\n",
    "    train_classifier(low_zero_aug_net, get_optimizer(low_zero_aug_net), aug_zer_data_loader, key=\"Low Zero Aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zer_raw:\n",
    "    device = torch.device(\"cuda\")\n",
    "    low_zero_net.eval()\n",
    "    test(low_zero_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zer_syn:\n",
    "    device = torch.device(\"cuda\")\n",
    "    low_zero_syn_net.eval()\n",
    "    test(low_zero_syn_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zer_aug:\n",
    "    device = torch.device(\"cuda\")\n",
    "    low_zero_aug_net.eval()\n",
    "    test(low_zero_aug_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, 117 / 118\n",
      "Loss:  0.1494552344083786\n"
     ]
    }
   ],
   "source": [
    "if nin_raw:\n",
    "    high_nine_net = build_classifier().cuda()\n",
    "    high_nine_net.train()\n",
    "    train_classifier(high_nine_net, get_optimizer(high_nine_net), mnist_high_nine, key=\"High Nine Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, 117 / 118\n",
      "Loss:  0.004706097766757011\n"
     ]
    }
   ],
   "source": [
    "if nin_syn:\n",
    "    nin_syn_net = build_classifier().cuda()\n",
    "    nin_syn_net.train()\n",
    "    train_classifier(nin_syn_net, get_optimizer(nin_syn_net), syn_nin_data_loader, key=\"High Nine Synth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nin_aug:\n",
    "    nin_aug_net = build_classifier().cuda()\n",
    "    nin_aug_net.train()\n",
    "    train_classifier(nin_aug_net, get_optimizer(nin_aug_net), aug_nin_data_loader, key=\"High Nine Aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4311/10000 (43%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if nin_raw:\n",
    "    device = torch.device(\"cuda\")\n",
    "    high_nine_net.eval()\n",
    "    test(high_nine_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nin_aug:\n",
    "    device = torch.device(\"cuda\")\n",
    "    nin_aug_net.eval()\n",
    "    test(nin_aug_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6301/10000 (63%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if nin_syn:\n",
    "    device = torch.device(\"cuda\")\n",
    "    nin_syn_net.eval()\n",
    "    test(nin_syn_net, device, balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'low_zero_syn_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-1e58cf4fb6a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnb_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlow_zero_syn_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'low_zero_syn_net' is not defined"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "model=low_zero_syn_net\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(balanced_test):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "confusion_matrix = confusion_matrix.numpy()\n",
    "np.fill_diagonal(confusion_matrix, 0)\n",
    "vis.PlotHeatMap(confusion_matrix, \"Low Zero Confusionsdf\", False)\n",
    "#y is correct, x is predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.ShowImages(format_to_image(synthesize_data_from_each_label(zer_gen, gaussian_noise, n_classes).cpu().detach(), n_classes, img_width), \"Fortnite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
