{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import average_precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from visdom import Visdom\n",
    "\n",
    "from lib.VisdomWrapper import *\n",
    "from lib.GANs import *\n",
    "from lib.DataCreationWrapper import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "file_path = \"input/creditcard.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "x = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:,-1:].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 21, shuffle = True, stratify = y)\n",
    "\n",
    "# Store Num Features\n",
    "n_features = x.shape[1]\n",
    "# n_noise_features = 10\n",
    "# num_fraud = np.sum(y_train)\n",
    "# num_valid = y_train.shape[0] - num_fraud\n",
    "\n",
    "# Center Mean and Unit Variance\n",
    "x_train = preprocessing.scale(x_train, axis = 0)\n",
    "x_test = preprocessing.scale(x_test, axis = 0)\n",
    "\n",
    "scalar = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "x_train = scalar.fit_transform(x_train)\n",
    "x_test = scalar.fit_transform(x_test)\n",
    "\n",
    "# #load GANS\n",
    "# FraudGen = GeneratorNetwork(n_noise_features, n_features)\n",
    "# FraudGen.load_state_dict(torch.load(\"models\\gen_nn1\"))\n",
    "# FraudGen.eval()\n",
    "# ValidGen = GeneratorNetwork(n_noise_features, n_features)\n",
    "# ValidGen.load_state_dict(torch.load(\"models\\gen_nn0\"))\n",
    "# FraudGen.eval()\n",
    "\n",
    "#Generate Data\n",
    "# balanced_x = np.concatenate((np.copy(x_train), synthesize_data(FraudGen, num_valid - num_fraud, gaussian_noise).detach().numpy()), axis=0)\n",
    "# balanced_y = np.concatenate((np.copy(y_train), np.ones((num_valid- num_fraud, 1))))\n",
    "\n",
    "\n",
    "\n",
    "# To Tensor\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_train = torch.from_numpy(y_train).double()\n",
    "y_test = torch.from_numpy(y_test).double()\n",
    "# balanced_x = torch.from_numpy(balanced_x).float()\n",
    "# balanced_y = torch.from_numpy(balanced_y).double()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)\n",
    "dtype = torch.float32\n",
    "\n",
    "# Creates Data Loaders\n",
    "ds_train = data_utils.TensorDataset(x_train, y_train)\n",
    "data_loader = data_utils.DataLoader(ds_train, batch_size=100, shuffle=True)\n",
    "# balanced_train = data_utils.TensorDataset(balanced_x, balanced_y)\n",
    "# balanced_loader = data_utils.DataLoader(balanced_train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "vis = VisdomController()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architechure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FraudClassifier, self).__init__()\n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.hidden3 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.hidden4 = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.hidden4(x)\n",
    "        x = self.out(x)\n",
    "        return x  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "BCE = nn.BCELoss()\n",
    "\n",
    "def train_classifier(network, optimizer, data, key):\n",
    "    network = network.to(device=device)\n",
    "    n = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for n_batch, (x, y) in enumerate(data):\n",
    "            n += 1\n",
    "            network.train()\n",
    "            x = x.to(device=device, dtype = dtype)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            t_start = millis = time.time()\n",
    "            \n",
    "            scores = network(x)\n",
    "            loss = BCE(scores, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Loss History\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            if (n_batch % 100 == 0):\n",
    "                display.clear_output(True)\n",
    "\n",
    "                # Basic Data            \n",
    "                print(\"Epoch {}, {} / {}\".format(epoch, n_batch, len(data)))\n",
    "                print(\"loss : \", loss)\n",
    "            \n",
    "                t_end = millis = time.time()\n",
    "                print(\"Time Elapsed : \", t_end - t_start)\n",
    "                print(n)\n",
    "                vis.PlotLoss(key, n , loss.item())\n",
    "            \n",
    "        #vis.Plot_Loss(\"Vanilla Loss\", epoch, epoch_loss.item() / len(data))\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, 2200 / 2279\n",
      "loss :  tensor(0.0008, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Time Elapsed :  0.004998683929443359\n",
      "11317\n"
     ]
    }
   ],
   "source": [
    "vanillaNN = FraudClassifier()\n",
    "vanillaOptim = optim.Adam(vanillaNN.parameters(), lr = 1e-5)\n",
    "\n",
    "train_classifier(vanillaNN, vanillaOptim, data_loader, \"Vanilla Data\")\n",
    "\n",
    "# balancedNN = FraudClassifier()\n",
    "# balancedOptim = optim.Adam(balancedNN.parameters(), lr = 1e-5)\n",
    "\n",
    "# train_classifier(balancedNN, balancedOptim, balanced_loader, \"Augment Balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec:\t 0.5130385003290211\n",
      "Recall:\t 0.5918367346938775\n",
      "F1:\t 0.703030303030303\n"
     ]
    }
   ],
   "source": [
    "vanillaNN.cuda()\n",
    "x_test = x_test.cuda()\n",
    "y_scores = vanillaNN(x_test)\n",
    "y_pred = np.round(y_scores.detach().cpu().numpy())\n",
    "average_precision = average_precision_score(y_test.cpu().numpy(), y_pred)\n",
    "print(\"Prec:\\t\", average_precision)\n",
    "print(\"Recall:\\t\", recall_score(y_test.cpu().numpy(), y_pred))\n",
    "print(\"F1:\\t\", f1_score(y_test.cpu().numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
